{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":441417,"sourceType":"datasetVersion","datasetId":200079}],"dockerImageVersionId":29661,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Import libraries","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport string\nimport numpy as np\nimport pandas as pd\nfrom string import digits\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\nimport logging\nimport tensorflow as tf\ntf.enable_eager_execution()\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nlogging.getLogger('tensorflow').setLevel(logging.FATAL)\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import train_test_split\nimport unicodedata\nimport io\nimport time\nimport warnings\nimport sys\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nPATH = \"../input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocess English and Hindi sentences","metadata":{}},{"cell_type":"code","source":"def unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn')\n\ndef preprocess_sentence(w):\n    w = unicode_to_ascii(w.lower().strip())\n    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n    w = re.sub(r'[\" \"]+', \" \", w)\n    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n    w = w.rstrip().strip()\n    return w\n\ndef hindi_preprocess_sentence(w):\n    w = w.rstrip().strip()\n    return w","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dataset(path=PATH):\n    lines=pd.read_csv(path,encoding='utf-8')\n    lines=lines.dropna()\n    lines = lines[lines['source']=='ted']\n    en = []\n    hd = []\n    for i, j in zip(lines['english_sentence'], lines['hindi_sentence']):\n        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n        en_1.append('<end>')\n        en_1.insert(0, '<start>')\n        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n        hd_1.append('<end>')\n        hd_1.insert(0, '<start>')\n        en.append(en_1)\n        hd.append(hd_1)\n    return hd, en","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def max_length(tensor):\n    return max(len(t) for t in tensor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Tokenization of the data","metadata":{}},{"cell_type":"code","source":"def tokenize(lang):\n  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n  lang_tokenizer.fit_on_texts(lang)\n  tensor = lang_tokenizer.texts_to_sequences(lang)\n  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n  return tensor, lang_tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_dataset(path=PATH):\n    targ_lang, inp_lang = create_dataset(path)\n    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)\nmax_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create Train and Test dataset","metadata":{}},{"cell_type":"code","source":"input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\nprint(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert(lang, tensor):\n  for t in tensor:\n    if t!=0:\n      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n    \nprint (\"Input Language; index to word mapping\")\nconvert(inp_lang, input_tensor_train[0])\nprint ()\nprint (\"Target Language; index to word mapping\")\nconvert(targ_lang, target_tensor_train[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create Dataset\n> We are using minimal configuration as the notebbok is not focussed on metrics performance but rather the implementation.","metadata":{}},{"cell_type":"code","source":"BUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 64\nsteps_per_epoch = len(input_tensor_train)//BATCH_SIZE\nembedding_dim = 128\nunits = 256\nvocab_inp_size = len(inp_lang.word_index)+1\nvocab_tar_size = len(targ_lang.word_index)+1\n\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Encoder","metadata":{}},{"cell_type":"code","source":"class Encoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n    super(Encoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.enc_units = enc_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.enc_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n\n  def call(self, x, hidden):\n    x = self.embedding(x)\n    output, state = self.gru(x, initial_state = hidden)\n    return output, state\n\n  def initialize_hidden_state(self):\n    return tf.zeros((self.batch_sz, self.enc_units))\n\nencoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Attention Mechanism","metadata":{}},{"cell_type":"code","source":"class BahdanauAttention(tf.keras.layers.Layer):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, query, values):\n    hidden_with_time_axis = tf.expand_dims(query, 1)\n    score = self.V(tf.nn.tanh(\n        self.W1(values) + self.W2(hidden_with_time_axis)))\n    attention_weights = tf.nn.softmax(score, axis=1)\n    context_vector = attention_weights * values\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n    return context_vector, attention_weights","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Decoder","metadata":{}},{"cell_type":"code","source":"class Decoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n    super(Decoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.dec_units = dec_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.dec_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc = tf.keras.layers.Dense(vocab_size)\n    self.attention = BahdanauAttention(self.dec_units)\n\n  def call(self, x, hidden, enc_output):\n    context_vector, attention_weights = self.attention(hidden, enc_output)\n    x = self.embedding(x)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n    output, state = self.gru(x)\n    output = tf.reshape(output, (-1, output.shape[2]))\n    x = self.fc(output)\n    return x, state, attention_weights\n\ndecoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Optimizer","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n  mask = tf.cast(mask, dtype=loss_.dtype)\n#   print(type(mask))\n  loss_ *= mask\n  return tf.reduce_mean(loss_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training\n\n>1. Pass *input* through *encoder* to get *encoder output*..\n>2. Then encoder output, encoder hidden state and the decoder input is passed to decoder.\n>3. Decoder returns *predictions* and *decoder hidden state*.\n>4. Decoder hidden state is then passed back to model.\n>5. Predictions are used to calculate loss.\n>6. Use *teacher forcing* (technique where the target word is passed as the next input to the decoder) for the next input to the decoder.\n>7. Calculate gradients and apply it to *optimizer* for backpropogation.","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n  loss = 0\n  with tf.GradientTape() as tape:\n    enc_output, enc_hidden = encoder(inp, enc_hidden)\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n    # Teacher forcing\n    for t in range(1, targ.shape[1]):\n      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n      loss += loss_function(targ[:, t], predictions)\n      dec_input = tf.expand_dims(targ[:, t], 1)\n\n  batch_loss = (loss / int(targ.shape[1]))\n  variables = encoder.trainable_variables + decoder.trainable_variables\n  gradients = tape.gradient(loss, variables)\n  optimizer.apply_gradients(zip(gradients, variables))      \n  return batch_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 10\n\nfor epoch in range(EPOCHS):\n  start = time.time()\n  enc_hidden = encoder.initialize_hidden_state()\n  total_loss = 0\n  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n    batch_loss = train_step(inp, targ, enc_hidden)\n    total_loss += batch_loss\n    if batch % 100 == 0:\n        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                     batch,\n                                                     batch_loss.numpy()))\n  if (epoch + 1) % 2 == 0:\n    checkpoint.save(file_prefix = checkpoint_prefix)\n\n  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                      total_loss / steps_per_epoch))\n  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(sentence):\n    attention_plot = np.zeros((max_length_targ, max_length_inp))\n    sentence = preprocess_sentence(sentence)\n    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                           maxlen=max_length_inp,\n                                                           padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n    result = ''\n    hidden = [tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n    for t in range(max_length_targ):\n        predictions, dec_hidden, attention_weights = decoder(dec_input,\n                                                             dec_hidden,\n                                                             enc_out)\n        predicted_id = tf.argmax(predictions[0]).numpy()\n        result += targ_lang.index_word[predicted_id] + ' '\n        if targ_lang.index_word[predicted_id] == '<end>':\n            return result, sentence\n        dec_input = tf.expand_dims([predicted_id], 0)\n    return result, sentence","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate(sentence):\n    result, sentence = evaluate(sentence)\n    print('Input: %s' % (sentence))\n    print('Predicted translation: {}'.format(result))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# restoring the latest checkpoint in checkpoint_dir\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"translate(u'politicians do not have permission to do what needs to be done.')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}